## Features

 - collect terms from a web page not just random words but actual vocabulary that appears in pages
   and jargon etc.

 - track visted pages


It can collect and archive the users search history.

It also keeps a collection of pages that I've crawled


Everything is wrapped in a session(

we start with a seed_query which kicks off our discovery of information.

next we have the seed_query_SERP_data which is the results we get from a browser. Once we have this information
we can begin to process this data.

SERP_data = [web_page_1, web_page_2, web_page_3]
SERP_data_visited = []

if(web_page.visited) {

	// Move the page into visited pages
}
)

### Scraping the SERP_data_visited

Once we have a collection of urls

We also need to track if a link is a sub-link of another page

SERP_data_visited = [
  {
    
  }
]



